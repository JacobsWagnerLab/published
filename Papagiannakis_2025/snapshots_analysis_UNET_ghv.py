#Python 3.7.0 (default, Jun 28 2018, 07:39:16)
#Type "copyright", "credits" or "license" for more information.
#
#IPython 7.8.0 -- An enhanced Interactive Python.

"""
Created on Mon Aug  5 14:32:50 2019

@author: Alexandros Papagiannakis, Christine Jacobs-Wagner lab, Sarafan ChEM-H, Stanford University, 2019
"""
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import skimage
from skimage import io, filters
from skimage.measure import label, regionprops
import scipy
from skimage.filters import threshold_otsu, threshold_local
from scipy import ndimage
from shapely.geometry import LineString
from PIL import Image
import pickle
from skimage.morphology import medial_axis
import os
from pims import ND2_Reader
from itertools import combinations


class unet_snapshots(object):
    """
    Developer: Alexandros Papagiannakis, Christine Jacobs-Wagner lab, Stanford University, 2020
    
    This class contains all the functions used for the analysis of the fluorescence statistics using
    the masked cell labels generated by neural networks. The masked labels are tiff images, with a unique
    pixel value (label) for each segmented cell instance. 
    
    Functions included in this class:
        __init__
            nd2_to_array
            unet_to_python
        show_unet_masks
        get_unet_mask
        get_cell_id_from_label
        get_bad_cells
        get_angle_from_slope
        get_medial_axis
            correct_angle_difference
            get_next_position
            recursive_medial_axis
        run_medial_axis
        cell_free_bkg_estimation
        back_sub
        get_background_corrected_images
        get_cell_mean_stats
        get_oned_coordinates
            get_pixel_projection
                get_relative_distance
        apply_oned_coordinates
    """
    def __init__(self, unet_path, snapshots_path, experiment, save_path):
        """
        The class is iniialized.
        
        Parameters
        ----------
        ___File paths___
        snapshots_paths: the path of the phase and signal images (e.g. HU-mCherry) snapshots. The path to the nd2 file
            A list of paths should be included with the right order (e.g. [phase_path, phase_after_path, signal_path] or [phase_path, signal_path])
            In the case of an empty lists, no snapshots are loaded
        unet_path: the path to the unet .tif cell labels
            If a non-valud file name is used no cell masks are loaded
            For instance, use 'none' to avoid loading cell masks
        save_path: path where the results are saved
        
        ___General parameters___
        experiment: a string describing the experiment (e.g. '07202919_seqA')
        The _init_ function contains a number of sub-functions which use the input parameters of the class
        
        Exceptions
        ----------
        raise ValueError if the XY positions do not include the same number of channels
        """
                   
        def nd2_to_array(images_path):
            """
            This function is used to convert .nd2 images to numpy arrays.
            It also returms the image metadata.
            
            Parameters
            ----------
            image_path - string: the path of the .nd2 file

            Returns
            -------
            [0] the iteration axis - string ('c', 't', 'mc' or 'mct')
            [1] the .nd2 metadata and images (from the ND2_Reader pims module). This is a class object and has multuiple functions
            [2] a dictionary which contains the images as numpy arrays organized by:
                iteration_axis 't' - For fast time lapse (stream acquisition), key2: frame
                iteration_axis 'c' - For snapshots of a single XY position, key1: channel
                iteration_axis 'mc' - For snapshots of multiple XY positions, key1: position, key2: channel
                iteration_axis = 'mct' - For time lapse across different channels, key1: position, key2: channel, key3: time-point
            [3] channels: list of strings - each string represents the channel (e.g. ['Phase', 'mCherry', 'GFP', 'Phase_after'])
                If a certain wavelength (lambda) is used two times in the ND acquisition, then the second channel instance is referred to as '_after'
                An empty list is returned if no channels are selected.
            [4] the number of time-points - positive integer or zero if the Time label was not selected in the ND acquisition
            [5] The number of XY positions - positive integer or zero if the XY label was not selected in the ND acquisition
            
            Notes
            -----
            This function was adapted to include all possible channel, time-point, xy-position permutations in our image acquisition protocols in NIS elements (including the JOBS module)
            New permutations may need to be included for new image permutations.
            The iteration axis determines how the image dimensions are iterated and stored into dictionaries
            """
            # The path of the .nd2 file 
            images = ND2_Reader(images_path)
            # "C:\Users\Alex\Anaconda3\Lib\site-packages\pims_nd2\nd2reader.py"
            # This path has been modified in lines 228 and 229 to accommodate the function.
            print('metadata:',images.metadata)
            print('dimensions:',images.sizes)
            
            scale = round(images.metadata['calibration_um'],3)  # Î¼m/px scale
            sensor = (images.sizes['x'], images.sizes['y'])
            channels = []
            if 'c' in images.sizes:
                # get the channels and frames from the .nd2 metadata
                number_of_channels = images.sizes['c']
                
                for i in range(number_of_channels):
                    ch = images.metadata['plane_'+str(i)]['name']
                    if ch in channels:
                        channels.append(ch+'_after')
                    else:
                        channels.append(ch)   
            # number_of_frames = images.metadata['sequence_count']
            iteration_axis = ''
            if 'm' in images.sizes and images.sizes['m'] > 1:
                iteration_axis += 'm'
                number_of_positions = images.sizes['m']
            if 'c' in images.sizes and images.sizes['c'] > 1:
                iteration_axis += 'c'
            if 't' in images.sizes and images.sizes['t'] > 1:
                iteration_axis += 't'
                number_of_timepoints = images.sizes['t']
            # For a stream acquisition
            if iteration_axis == 't':
                image_arrays = {}
                number_of_positions = 0
                with images as frames:
                    t = 0 # time point
                    print(frames)
                    frames.iter_axes = iteration_axis
                    for frame in frames:
                        image_arrays[t] = np.array(frame)
                        t += 1
                frames.close()
            # For snapshots at different channels
            elif iteration_axis == 'c':
                image_arrays = {}
                number_of_timepoints = 0
                number_of_positions = 0
                with images as frames:
                    i = 0
                    print(frames)
                    frames.iter_axes = iteration_axis
                    for frame in frames:
                        image_arrays[channels[i]] = np.array(frame)
                        i += 1
                frames.close()
            # For snapshots at different XY positions for a single channel (this is how JOBS extracts the snapshots)
            elif iteration_axis == 'm':      
                image_arrays = {}
                number_of_timepoints = 0
                number_of_channels = 1
                with images as frames:
                    i = 0
                    print(frames)
                    frames.iter_axes = iteration_axis
                    for frame in frames:
                        image_arrays[i] = np.array(frame)
                        i += 1
                frames.close()
            # For snapshots at different channels and XY positions
            elif iteration_axis == 'mc':
                image_arrays = {}
                number_of_timepoints = 0
                with images as frames:
                    print(frames)
                    frames.iter_axes = iteration_axis
                    pos = 0
                    ch = 0
                    image_arrays[pos] = {}
                    for frame in frames:
                        if ch < number_of_channels:
                            if pos < number_of_positions:
                                image_arrays[pos][channels[ch]] = np.array(frame)
                                ch+=1
                        elif ch == number_of_channels:
                            pos += 1
                            image_arrays[pos] = {}
                            ch = 0
                            image_arrays[pos][channels[ch]] = np.array(frame)
                            ch+=1
                frames.close()
            # For snapshots at different channels and XY positions and timepoints
            elif iteration_axis == 'mt':
                image_arrays = {}
                with images as frames:
                    print(frames)
                    frames.iter_axes = iteration_axis
                    pos = 0
                    tm = 0
                    image_arrays[pos] = {}
                    for frame in frames:
                        if tm < number_of_timepoints:
                            image_arrays[pos][tm] = np.array(frame)
                            tm+=1
                        elif tm == number_of_timepoints:
                            tm = 0
                            if pos < number_of_positions-1:
                                pos += 1
                                image_arrays[pos] = {}
                                image_arrays[pos][tm] = np.array(frame)
                                tm+=1             
                frames.close()
            # For snapshots at different channels and XY positions and timepoints
            elif iteration_axis == 'mct':
                image_arrays = {}
                with images as frames:
                    print(frames)
                    frames.iter_axes = iteration_axis
                    pos = 0
                    ch = 0
                    tm = 0
                    image_arrays[pos] = {}
                    image_arrays[pos][channels[ch]] = {}
                    for frame in frames:
                        if tm < number_of_timepoints:
                            image_arrays[pos][channels[ch]][tm] = np.array(frame)
                            tm+=1
                        elif tm == number_of_timepoints:
                            tm = 0
                            if ch < number_of_channels-1:
                                ch += 1
                                image_arrays[pos][channels[ch]] = {}
                                image_arrays[pos][channels[ch]][tm] = np.array(frame)
                                tm+=1
                            elif ch == number_of_channels-1:
                                ch = 0
                                pos+=1
                                image_arrays[pos] = {}
                                image_arrays[pos][channels[ch]] = {}
                                image_arrays[pos][channels[ch]][tm] = np.array(frame)
                                tm+=1
                frames.close()
            # if no channels or time points are specified there should be only one image
            elif iteration_axis == '':
                number_of_timepoints = 0
                number_of_positions = 0
                with images as frames:
                    for frame in frames:
                        image_arrays = np.array(frame)
            
            return iteration_axis, images, image_arrays, channels, number_of_timepoints, number_of_positions, scale, sensor


        def unet_to_python(unet_path, experiment, position, pad=5):
            """
            This function incoorporates the masks from Unet to python
            
            Parameters
            ----------
            unet_path: string - the path of the tif image of the cell masks returned by the Unet
            experiment: the experiment ID string
            position: integer corresponding to the XY position (starting at zero)
            pad: integer - the size of the frame around the cells used to crop the cell masks
            
            Returns
            -------
            [0] cropped_cell_masks: a dictionary which inlcudes the cropped cell masks. A cropping pad of 3 pixels is used
            [1] pads: a tuple of 4 coordinates corresponding to the croping rectangle around the cell mask -> (miny, maxy, minx, maxx)
            """
            # check for bad cells in the results destination
            cell_count = 0
            
            if position < 9:
                position_string = 'xy0'+str(position+1)
            elif position >= 9:
                position_string = 'xy'+str(position+1)

            mask_array = io.imread(unet_path)
            cropped_cell_masks = {}
            pads = {}
            for cell in range(1, mask_array.max()+1):
                cell_id = experiment+'_'+position_string+'_'+str(cell)
                cell_mask = np.zeros((mask_array.shape[0], mask_array.shape[1]))
                cell_mask[mask_array==cell]=1
                cell_mask = ndimage.morphology.binary_fill_holes(cell_mask).astype(int)
                y_mask_coords, x_mask_coords = np.where(cell_mask==1)
                minx,miny,maxx,maxy = x_mask_coords.min(), y_mask_coords.min(), x_mask_coords.max(), y_mask_coords.max()
                # remove the masks at the edge of the sensor
                if maxy < mask_array.shape[0]-pad and maxx < mask_array.shape[1]-pad and miny > pad and minx > pad:
                    cropped_cell_masks[cell_id] = cell_mask[(miny-pad):(maxy+pad), (minx-pad):(maxx+pad)]
#                    plt.imshow(cropped_cell_masks[cell_id])
#                    plt.show()
                    pads[cell_id] = (minx-pad, miny-pad, maxx+pad, maxy+pad)
                    cell_count +=1
            print(cell_count, 'cells.')
            return cropped_cell_masks, pads

        dir_list = os.listdir(snapshots_path)
        if '.DS_Store' in dir_list:
            dir_list.remove('.DS_Store')
        channels = []
        for img in dir_list:
            if img[img.find('Channel')+7:img.find('_Seq')] not in channels:
                channels.append(img[img.find('Channel')+7:img.find('_Seq')])
        print(len(channels), 'channels detected:', channels)
        if len(dir_list)%len(channels) == 0:
            print(int(len(dir_list)/len(channels)),'xy positions detected')
        else:
            raise ValueError('There is not the same number of channels per xy position')
        
        image_arrays = {}
        for ch in channels:
            image_arrays[ch]={}
        for pos in range(int(len(dir_list)/len(channels))):
            cor_pos = pos*len(channels)
            for ch_pos_index in range(len(channels)):
                cor_pos_str = 'Seq'+'0'*(4-len(str(cor_pos)))+str(cor_pos) 
                image_string =  [s for s in dir_list if cor_pos_str in s][0]
                pos_channel = [ch for ch in channels if(ch in image_string)]
                print(pos_channel[0], image_string, pos)
                image_arrays[pos_channel[0]][pos] = nd2_to_array(snapshots_path+'/'+image_string)
                cor_pos+=1
        
        if experiment+'_cropped_masks' not in os.listdir(save_path):
            print('did not find the cropped masks...generating...')
            mask_arrays = {}
#            ch = 'Trans'
            for pos in range(int(len(dir_list)/len(channels))):
                cor_pos = pos*len(channels)
                cor_pos = 'Seq'+'0'*(4-len(str(cor_pos)))+str(cor_pos) 
                image_string =  [s for s in dir_list if cor_pos in s][0]
                print(image_string[0:-4]+'_mask.tif')
                mask_arrays[pos] = unet_to_python(unet_path+'/'+image_string[0:-4]+'_mask.tif', experiment, pos, pad=5)
            with open(save_path+'/'+experiment+'_cropped_masks', 'wb') as handle:
                pickle.dump(mask_arrays, handle)
        else:
            print('found the cropped masks...loading')
            with open(save_path+'/'+experiment+'_cropped_masks', 'rb') as handle:
                mask_arrays = pickle.load(handle)

        self.image_arrays = image_arrays
        self.mask_arrays = mask_arrays
        self.experiment = experiment
        self.save_path = save_path
        self.channels = list(self.image_arrays.keys())
        self.sensor =  self.image_arrays[self.channels[0]][0][-1] 
        self.unet_path = unet_path
        
        def count_cells(mask_arrays):
            cell_count = 0
            for pos in mask_arrays:
                cell_count+=len(mask_arrays[pos][0])
            return cell_count
    
        cell_count = count_cells(self.mask_arrays)
        self.cell_count = cell_count
        print(self.cell_count, 'total segmented cells...')
            

    # CHECK THE UNET SEGMENTATION 
    def show_unet_masks(self, save=False, curate=False):
    # def show_oufti_meshes(self, channel_offsets):
        """
        This function can be used to plot the perimeter of the Unet masks after processing.
        The perimeter of the masks is plotted over the phase image.
        """
        
        bad_cell_coordinates = {}
        
        for pos in self.mask_arrays:
            print('printing the cell boundaries on the phase contrast image...')
            plt.figure(figsize=(15,15))
            plt.imshow(self.image_arrays['Trans'][pos][2])
            for test_cell in self.mask_arrays[pos][0]:
                cropped_mask = self.mask_arrays[pos][0][test_cell]
                dilated_cell_mask = scipy.ndimage.morphology.binary_dilation(cropped_mask, iterations=1)
                cell_mesh_y, cell_mesh_x = zip(*skimage.measure.find_contours(dilated_cell_mask, level=0.5)[0])
                cell_pad = self.mask_arrays[pos][1][test_cell]
                plt.plot(cell_mesh_x+cell_pad[0], cell_mesh_y+cell_pad[1], color='white')
            if save==True:
                plt.savefig(self.save_path+'/segmentation_'+self.experiment+'_xy'+str(pos+1)+'.jpeg')
            if curate==True:
                bad_cell_coordinates[pos] = plt.ginput(n=0, timeout=0)
                print('bad cells in locations:',bad_cell_coordinates[pos])
            else:
                bad_cell_coordinates[pos] = 'none'
            plt.show()
            plt.close()
            
        with open(self.save_path+'/'+self.experiment+'_bad_cell_coordinates', 'wb') as handle:
            pickle.dump(bad_cell_coordinates, handle, protocol=pickle.HIGHEST_PROTOCOL)
            
            
    def get_unet_mask(self, position):
        
        unet_dir = os.listdir(self.unet_path)
        cor_pos = position*len(self.channels)
        cor_pos = 'Seq'+'0'*(4-len(str(cor_pos)))+str(cor_pos) 
        image_string =  [s for s in unet_dir if cor_pos in s][0]
        single_unet_path = self.unet_path+'/'+image_string[0:-4]+'_mask.tif'
        mask_labels = io.imread(single_unet_path)
        return mask_labels
    
    
    def get_cell_id_from_label(self, mask_label, position_string):
        
        return self.experiment+'_'+position_string+'_'+str(mask_label)
            
    
    def get_bad_cells(self):
        
        bad_cells = []
        
        with open (self.save_path+'/'+self.experiment+'_bad_cell_coordinates', 'rb') as handle:
            bad_cell_coordinates = pickle.load(handle)
        print('getting bad cell IDs:')
        

        for pos in bad_cell_coordinates:
            
            if pos < 9:
                position_string = 'xy0'+str(pos+1)
            elif pos >= 9:
                position_string = 'xy'+str(pos+1)
            
            mask_labels = self.get_unet_mask(pos)  
          
            for bd_crd in bad_cell_coordinates[pos]:
                bad_label = mask_labels[int(bd_crd[1]),int(bd_crd[0])]
                if bad_label>0:
                    # bad_cell_id = self.experiment+'_'+position_string+'_'+str(bad_label)
                    bad_cell_id = self.get_cell_id_from_label(bad_label, position_string)
                    print(bad_cell_id)
                    plt.imshow(self.mask_arrays[pos][0][bad_cell_id])
                    plt.show()
                    bad_cells.append(bad_cell_id)
                    
        with open(self.save_path+'/'+self.experiment+'_bad_cells', 'wb') as handle:
            pickle.dump(bad_cells, handle, protocol=pickle.HIGHEST_PROTOCOL)
                
                    

     # MEDIAL AXIS ESTIMATION #
    def get_angle_from_slope(self, displacements, slope='none'):
        """
        This function is used to estimate the angle of each microtubule displacement from two adjacent time-points.
        The angle is estmated using the slope of the microtubule displacement and the orientation (negative or positive dx and dy)
        
        Specifically, the slope is converted to an angle.
        Using the dx and dy displacements it is decided to which of the quartiles in a circle the angle belongs and the appropriate adjustments are made.
        
        Parameters
        ----------
        displacements: tuple - (dx, dy). The displacement between two linked spots in the x and y direction
        slope: float - the slope of the displacement
               if slope == 'none' the slope is estimated as dy/dx
        
        Returns
        -------
        angle: float - the angle of the displacement in degrees
        
        Notes
        -----
        This function is used for the medial axis extimation: self.get_medial_axis()
        """
        dx = displacements[0]
        dy = displacements[1]
        if dx != 0:
            if slope =='none':
                slope = dy/dx
            angle = np.rad2deg(np.arctan(slope))
            if angle >= 0:
                angle = angle
            elif angle < 0:
                angle  = 360 + angle
            if slope >= 0:
                if dx >= 0:
                    angle = angle
                elif dx < 0:
                    angle = 180 + angle
            elif slope < 0:
                if dx >= 0:
                    angle = angle
                elif dx< 0:
                    angle = angle - 180
        elif dx == 0:
            if dy > 0:
                angle = 90
            elif dy < 0:
                angle = 270
            elif dy == 0:
                angle = 0
        return angle


    def get_medial_axis(self, cropped_cell_mask, cell_pad, radius_px=8, half_angle=22, cap_knot=13, max_degree=60):
        """
        This function construct the medial axis of a signle cell, 
        as well as the relative coordinates of the cell from one pole to the other.
        
        Parameters
        ----------
        the cell ID (date_xyPosition_cellNumber produced after initializing the class)
        radius_px: positive integer - the radius beyond which the algorithm searches for the next anchor point in the circle sector
        half_angle: positive_float - the half angle of the circle sector within which the next anchor point is searched for.
            e.g. an angle of 22 degrees indicates that the next angle will be searched for in the secotr of 44 degrees (quartile), 22 degrees adjuscent the previous orientation
        cap_px: positive_interger - the number of knots excluded at the poles. This region will be extended using the anlge from the previous 10 anchors. 
        max_degree: positive_integer - the maximum degree of the fitted polynomials
        
        Returns
        -------
        [0] A pandas DataFrame including the absolute, scaled and relative coordinates of the medial axis
            Columns:
                'cropped_x': the cropped x coordinates of the medial axis (cropped by the cell pad)
                'cropped_y': the croppedyx coordinates of the medial axis (cropped by the cell pad)
                'arch_length': the arch length of the medial axis along the cell length
                'arch_length_centered': the arch length of the medial axis scaled by the centroid
                'arch_length_scaled': the relative arch length from -1 to 1
        [1] The x,y coordinates of the cell centroid
        [2] The croped x,y coordinates of the cell centroid, cropped by the cell pad
        """

        # GET THE CELL MASK AND THE DISTANCE TRANSFORMATION OF THE CELL MASK
        cell_mask = cropped_cell_mask
        # get the cropped mask of the single cell
#        dilated_cell_mask =  scipy.ndimage.morphology.binary_dilation(cell_mask, iterations=2)
        # get the resized cell mask
        resized_cell_mask = np.array(Image.fromarray(cell_mask).resize((cell_mask.shape[1]*10, cell_mask.shape[0]*10), resample=Image.NEAREST))
        skel, dist = medial_axis(resized_cell_mask, return_distance=True)
#        dist = dist * (dist>5) # to set a distance threshold in the distance transformed mask
    
#        plt.figure(figsize=(10,8))
#        plt.imshow(dist)
#        plt.plot(np.nonzero(skel)[1], np.nonzero(skel)[0], 'o', markersize=0.2)
#        plt.show()
        
        # GET THE FIRST ANCHOR POINT AT THE CENTER OF THE MAX LENGTH DIMENSION
        # CORRESPONDING TO THE POINT WITH THE MAXIMUM DISTANCE FROM THE CELL EDGES
        # THE ANGLE OF THE CELL AT THE FIRST ANCHOR POINT IS ALSO ESTIMATED
        len_y = len(dist)
        len_x = len(dist[0])
        length = (len_x, len_y)
        # the index of the longest coordinate (x or y)
        max_index = length.index(max(length))
        if max_index == 0:
            half_x = int(round(len_x/2 ,0))
            half_y = np.argmax(np.transpose(dist)[half_x])
        elif max_index == 1:
            half_y = int(round(len_y/2, 0))
            half_x = np.argmax(dist[half_y])
            
        start_x = half_x
        start_y = half_y
        cropped_window = dist[(start_y-10):(start_y+11), (start_x-10):(start_x+11)]
#        plt.imshow(cropped_window)
#        plt.show()
        window_df = pd.DataFrame()
        window_df['x'] = np.nonzero(cropped_window)[1]
        window_df['y'] = np.nonzero(cropped_window)[0]
        window_df['fluor'] = cropped_window[np.nonzero(cropped_window)].ravel()
        window_df['distance'] = np.sqrt((window_df.x-10)**2 + (window_df.y-10)**2)
        window_df = window_df[window_df.distance>5]
        window_df = window_df[window_df.fluor == window_df.fluor.max()]
        if window_df.shape[0] == 1:
            start_angle = self.get_angle_from_slope((window_df.x.values[0]-10, window_df.y.values[0]-10))
        elif window_df.shape[0] > 1:
            window_df = window_df[window_df.distance == window_df.distance.max()]
            start_angle = self.get_angle_from_slope((window_df.x.values[0]-10, window_df.y.values[0]-10))
#        print(start_angle)
        
        # THIS CODE CORRECTS THE ANGLE DIFFERENCE
        def correct_angle_difference(source_angle, destination_angle):
            """
            This function is used to correct the difference between two angles.
            It returns a positive angle smaller than 180 degrees.
            """
            a = destination_angle - source_angle
            if a >= 180:
                return 360-a
            elif a <= -180:
                return 360+a
            else:
                return abs(a)
         
        # THIS CODE ESTIMATES THE NEXT ANCHOR POINT USING THE PREVIOUS ONE AND THE ANGLE
        def get_next_position(dist, x, y, angle, list_of_knots):
            """
            This function scans the cell mask distance transformation for the max distance knots
            that will be used to fit the medial axis.
            
            The knots have a resolution of 0.1 pixels.
            """
            dist_temp = dist.copy()
            dist_temp[y][x]=0
            # radius_px = 8 # good parameter
            crop_dist = dist_temp[(y-radius_px):(y+radius_px+1), (x-radius_px):(x+radius_px+1)]
#            col = np.argmax(np.amax(crop_dist, axis=1))
#            row = np.argmax(np.amax(crop_dist, axis=0))
            y_coords, x_coords = np.nonzero(crop_dist)
            intensities = crop_dist[np.nonzero(crop_dist)]
            
            intensity_df = pd.DataFrame()
            intensity_df['x'] = x_coords + x -radius_px
            intensity_df['y'] = y_coords + y -radius_px
            intensity_df['fluor'] = intensities
            
            intensity_df['dx'] =  intensity_df['x']-x
            intensity_df['dy'] =  intensity_df['y']-y
            intensity_df['distance'] = np.sqrt(intensity_df.dx**2 + intensity_df.dy**2)
            intensity_df['angle'] = intensity_df.apply(lambda row: self.get_angle_from_slope((row.dx, row.dy)), axis=1)
            intensity_df['angle_dif'] = intensity_df.apply(lambda row: correct_angle_difference(angle, row.angle), axis=1)
            
#            intensity_df = intensity_df[intensity_df.angle_dif<=45] 
            intensity_df = intensity_df[(intensity_df.angle_dif<=half_angle) & (intensity_df.distance>6)] 
            
            if intensity_df.shape[0]>0:
                max_df = intensity_df[intensity_df.fluor == intensity_df.fluor.max()] #new
#                max_df = intensity_df[intensity_df.angle_dif == intensity_df.angle_dif.min()]
                if max_df.shape[0] > 0:
                    if max_df.shape[0] > 1:
    #                    max_df['distance'] = np.sqrt(max_df.dx**2 + max_df.dy**2)
    #                    max_df = max_df[max_df.distance==max_df.distance.min()]
                        max_df = max_df[max_df.angle_dif==max_df.angle_dif.min()] #new
#                        max_df = max_df[max_df.fluor==max_df.fluor.max()] # old
    #                max_index = max_df.index[0]
                    new_x = max_df.x.values[0]
                    new_y = max_df.y.values[0]
                    new_angle = max_df.angle.values[0]
                    max_fluor = max_df.fluor.values[0]
    #                print(max_fluor)
                
                    if (new_x, new_y) not in list_of_knots:
                        if max_fluor >= 3:
    #                        print(new_x,  new_y, new_angle)
                            return new_x,  new_y, new_angle
                        elif max_fluor < 3:
                            print('This is the end of the cell:', index_increment)
                            return False
                    elif (new_x, new_y) in list_of_knots:
                        print('This is the end of the cell and a loop is formed:', index_increment)
                        return 'loop'
                else:
                    print('This is the end of the cell:', index_increment)
                    return False
            elif intensity_df.shape[0]==0:
                print('This is the end of the cell:', index_increment)
                return False
        
        
        # RECURSIVE ALGORITHM TO GET ALL ANCHOR POINTS
        def recursive_medial_axis(index_increment, dist, x, y, angle, index, list_of_knots):
            """
            This is a function that runs the "get_next_position" finction recursively.
            """
            new_knot =get_next_position(dist, x, y, angle, list_of_knots)
            if new_knot != False:
                if new_knot != 'loop':
                    new_x,new_y,new_angle = new_knot
                    list_of_knots.append((new_x, new_y))
    #                print(new_x, new_y, new_angle)
                    if index_increment == 1:
                        index += 1
                    if index_increment == -1:
                        index -= 1
                    xyz_coord_list.append((new_x, new_y, index))
                    
                    x_list, y_list, z_list = list(zip(*xyz_coord_list))
                    pre_df = pd.DataFrame()
                    pre_df['x'] = np.array(x_list)/10
                    pre_df['y'] = np.array(y_list)/10
                    pre_df['z'] = z_list
                    pre_df = pre_df.sort_values(['z'])
        
                    line_coords = list(map(lambda x, y:(x,y), pre_df.x, pre_df.y))
                    line = LineString(line_coords)
    #                input()
                    if line.is_simple == True:
                        recursive_medial_axis(index_increment, dist, new_x, new_y, new_angle, index, list_of_knots)
                    elif line.is_simple == False:
                        print('This is the end of the cell and a loop is formed...')
                # remove the loops
                elif new_knot == 'loop':
                    for i in range(20):
                         xyz_coord_list.pop()
                
        # Run the recursive algorithms to get the anchor points for the central line fit
        index_increment = 1 # run towards one dimension
        index = 0
        list_of_knots = [(start_x, start_y)]
        xyz_coord_list = [(start_x, start_y, index)]
        x = start_x
        y = start_y
        angle = start_angle
        recursive_medial_axis(index_increment, dist, x, y, angle, index, list_of_knots)
        xyz_coord_list_1 = xyz_coord_list.copy()
        index_increment = -1 # run towards the opposite dimension
        index = 0
        list_of_knots = [(start_x, start_y)]
        xyz_coord_list = [(start_x, start_y, index)]
        x = start_x
        y = start_y
        angle = start_angle + 180
        if angle >= 360:
            angle = angle-360
        recursive_medial_axis(index_increment, dist, x, y, angle, index, list_of_knots)
        
        xyz_coord_list = xyz_coord_list_1 + xyz_coord_list[1:] # combine the two lists of coordinates
        
        # GETTING THE XY COORDINATES OF THE ANCHOR POINTS
        # getting the x,y and z coordinates of the knots
        x_list, y_list, z_list = list(zip(*xyz_coord_list))
#        plt.figure(figsize=(10,10))
#        plt.imshow(resized_cell_mask)
#        plt.plot(x_list, y_list, 'o', markersize=1)
#        plt.show()
#        plt.figure(figsize=(10,10))
#        plt.scatter(x_list, y_list, c=z_list)
#        plt.show()
        # CHECKING IF THE CENTRAL LINE INTERSECTS ITSELF AND REMOVING THE LOOPS
        # rescaling and sorting the coordinates of the knots
        pre_df = pd.DataFrame()
        pre_df['x'] = np.array(x_list)/10
        pre_df['y'] = np.array(y_list)/10
        pre_df['z'] = z_list
        pre_df = pre_df.sort_values(['z'])
        line_coords = list(map(lambda x, y:(x,y), pre_df.x, pre_df.y))
        line_coords = line_coords[::2]
        line = LineString(line_coords)
        positive_intersection = False
        negative_intersection = False
        if line.is_simple == False:
            print('removing the loop...')
            list_of_lines = []
            line_intersections = []
            for pnt in range(len(line_coords)-1):
                list_of_lines.append(LineString(line_coords[pnt:pnt+2]))
            for l1, l2 in combinations(list_of_lines,2): #For all combinations of segments
#                print(l1.intersection(l2).coords[:])
                if l1.crosses(l2) == True: #Find crossings
#                    print('cross')
                    line_intersections.append(l1.intersection(l2).coords[:][0])
            intersection_points_positive = []
            intersection_points_negative = []
            for intersect in line_intersections:
                distance_df = pre_df.copy()
                distance_df['inter_distance'] = np.sqrt((distance_df.x - intersect[0])**2+(distance_df.y - intersect[1])**2)
                distance_df = distance_df.sort_values(['inter_distance'])
                distance_df = distance_df[0:4]
                intersection_point = distance_df[distance_df.z.abs()==distance_df.z.abs().min()].z.values[0]
                if intersection_point > 0:
                    intersection_points_positive.append(intersection_point)
                elif intersection_point < 0:
                    intersection_points_negative.append(intersection_point)
            if len(intersection_points_positive) > 0:
                pre_df = pre_df[pre_df.z < intersection_points_positive[np.argmax(intersection_points_positive)]]
                positive_intersection = True
            elif len(intersection_points_negative) > 0:
                pre_df = pre_df[pre_df.z > intersection_points_negative[np.argmin(intersection_points_negative)]]
                negative_intersection = True
#        plt.imshow(resized_cell_mask)
#        plt.plot(pre_df.x*10, pre_df.y*10, 'o')
#        plt.show()
                
        # TRUNCATE THE MEDIAL AXIS COORDINATES FROM THE EDGES
        if pre_df.shape[0]>2*cap_knot+5:
            pre_df_max_index = cap_knot
        elif pre_df.shape[0]<=2*cap_knot+5:
            pre_df_max_index = pre_df.shape[0]/2 - 5

        if positive_intersection == False and negative_intersection == False:
            truncated_df = pre_df[pre_df_max_index:-pre_df_max_index]
        elif positive_intersection == False and negative_intersection == True:
            truncated_df = pre_df[0:-pre_df_max_index]
        elif positive_intersection == True and negative_intersection == False:
            truncated_df = pre_df[pre_df_max_index:]
        elif positive_intersection == True and negative_intersection == True:
            truncated_df = pre_df
#        plt.imshow(cell_mask)
#        plt.plot(truncated_df.x, truncated_df.y, 'o', markersize=0.2)
#        plt.show()
        
        # EXTENDING THE CENTRAL LINE AT THE EDGES USING THE AVERAGE ANGLE FROM THE 10 PREVIOUS ANCHORS
        # For the negative side
        if truncated_df.shape[0] >= 10:
            trunc_index = 10
        elif truncated_df.shape[0] < 10:
            trunc_index = truncated_df.shape[0]
        
        slope_1_df = truncated_df[0:trunc_index]
        slope_1 = np.polyfit(slope_1_df.x, slope_1_df.y, 1)[0]
        x_1 = np.array(slope_1_df.x)
        y_1 = np.array(slope_1_df.y)
        dx_1 = round(x_1[0] - x_1[trunc_index-1], 0)
        dy_1 = round(y_1[0] - y_1[trunc_index-1],0)

        angle_1 = self.get_angle_from_slope((dx_1, dy_1), slope_1)
        
        # For the positive side
        slope_2_df = truncated_df[-trunc_index:]
        slope_2 = np.polyfit(slope_2_df.x, slope_2_df.y, 1)[0]
        x_2 = np.array(slope_2_df.x)
        y_2 = np.array(slope_2_df.y)
        dx_2 = round(x_2[trunc_index-1] - x_2[0],0)
        dy_2 = round(y_2[trunc_index-1] - y_2[0],0)
        angle_2 = self.get_angle_from_slope((dx_2, dy_2), slope_2)
        
        min_z = truncated_df.z.min()
        max_z = truncated_df.z.max()
        
        x_list_1 = [x_1[0]]
        y_list_1 = [y_1[0]]
        z_list_1 = [min_z]
        
        # extend towards the negative side using the average angle at the edge of the central line
        for i in range(55):
            x_list_1.append(x_list_1[-1]+0.5*np.cos(angle_1*np.pi/180))
            y_list_1.append(y_list_1[-1]+0.5*np.sin(angle_1*np.pi/180))
            z_list_1.append(z_list_1[-1]-1)
        
        x_list_2 = [x_2[-1]]
        y_list_2 = [y_2[-1]]
        z_list_2 = [max_z]
        
        # extend towards the positive side using the average angle at the edge of the central line
        for i in range(55):
            x_list_2.append(x_list_2[-1]+0.5*np.cos(angle_2*np.pi/180))
            y_list_2.append(y_list_2[-1]+0.5*np.sin(angle_2*np.pi/180))
            z_list_2.append(z_list_2[-1]+1)
            
        x_list_final = x_list_1[1:]+x_list_2[1:]
        y_list_final = y_list_1[1:]+y_list_2[1:]
        z_list_final = z_list_1[1:]+z_list_2[1:]
        
        pre_df_2 = pd.DataFrame()
        pre_df_2['x'] = x_list_final
        pre_df_2['y'] = y_list_final
        pre_df_2['z'] = z_list_final
        pre_df = pd.concat([truncated_df, pre_df_2])
        pre_df = pre_df.sort_values(['z'])
#        plt.imshow(cell_mask)
#        plt.plot(pre_df.x, pre_df.y)
#        pre_df = pre_df[50:-50]
        # a bivariate spline is fitted to the data
#        tck, u = interpolate.splprep([pre_df.x, pre_df.y, pre_df.z], k=1, s=50)
##        tck, u = interpolate.splprep([pre_df.x, pre_df.y, pre_df.z], k=1, s=30)
#        x_knots, y_knots, z_knots = interpolate.splev(tck[0], tck)
##        u_fine = np.linspace(-1,2,pre_df.shape[0]*100+1000)
#        u_fine = np.linspace(-2,3,pre_df.shape[0]*300)
#        x_hat, y_hat, z_hat = interpolate.splev(u_fine, tck)
        
        # FIT A nth DEGREE POLYNOMIAL TO THE EXTENDED CENTRAL LINES
        # use this code if a polynomial fit is preferred versus a bivariate spline
        polynomial_degree = int(pre_df.shape[0]/10-5)
        if polynomial_degree > max_degree:
            polynomial_degree = max_degree
        print('fitting polynomial fucntions of degree:', polynomial_degree)
        extended_z = np.arange(pre_df.z.min(), pre_df.z.max(), 0.01)
        fit_x = np.polyfit(pre_df.z, pre_df.x, polynomial_degree) #nth degree polynomial fit long the X axis
        x_hat = np.polyval(fit_x, extended_z)
#        plt.plot(pre_df.z, pre_df.x, 'o')
#        plt.plot(extended_z, x_hat)
#        plt.show()
        fit_y = np.polyfit(pre_df.z, pre_df.y, polynomial_degree) #nth degree polynomail fit along the Y axis
        y_hat = np.polyval(fit_y, extended_z)
#        plt.plot(pre_df.z, pre_df.y, 'o')
#        plt.plot(extended_z, y_hat)
#        plt.show()
        # REMOVE THE CENTRAL LINE COORDINATES THAT DO NOT FALL INTO THE CELL MASK
        # getting only those coordinates of the medial axis that fall into the original cell mask
        x_fine_round = np.around(x_hat,0).astype(int)
        y_fine_round = np.around(y_hat,0).astype(int)
        good_indexes = (x_fine_round<cell_mask.shape[1])*(y_fine_round<cell_mask.shape[0])
        good_indexes_2 = (x_fine_round>=0)*(y_fine_round>=0)
        good_indexes= good_indexes * good_indexes_2
        x_fine = x_hat[good_indexes]
        y_fine = y_hat[good_indexes]
        x_fine_round = x_fine_round[good_indexes]
        y_fine_round = y_fine_round[good_indexes]
        nonzero_medial_indexes = np.nonzero(cell_mask[y_fine_round, x_fine_round])     
        x_fine_good = x_fine[nonzero_medial_indexes]
        y_fine_good = y_fine[nonzero_medial_indexes]
#        plt.imshow(cell_mask)
#        plt.plot(x_fine_good, y_fine_good)
#        plt.show()
        # GENERATE THE RELATIVE CELL COORDINATES AND THE CENTROID
        # generate the medial axis dataframe
        medial_axis_df = pd.DataFrame()
        medial_axis_df['cropped_x'] = x_fine_good
        medial_axis_df['cropped_y'] = y_fine_good
        # get the arch length of the medial axis
        delta_x_sqr = (x_fine_good[1:] - x_fine_good[0:-1])**2
        delta_y_sqr = (y_fine_good[1:] - y_fine_good[0:-1])**2
        disp_array = np.sqrt(delta_x_sqr + delta_y_sqr)
        disp_list = [0]
        for disp in disp_array:
            disp_list.append(disp_list[-1]+disp)
        medial_axis_df['arch_length'] = disp_list 
        medial_axis_df['arch_length_centered'] = disp_list - np.max(disp_list)/2
        medial_axis_df['arch_length_scaled'] = medial_axis_df['arch_length_centered'] / medial_axis_df['arch_length_centered'].max()
        # get the cropped centroid of the medial axis
        center_df = medial_axis_df[medial_axis_df.arch_length_centered.abs()==medial_axis_df.arch_length_centered.abs().min()]
        cropped_centroid = (center_df.cropped_x.mean(), center_df.cropped_y.mean())
        # PLOT THE CELL MASK WITH THE CENTRAL LINE AND THE CENTROID
        # plot and medial axis on the cell mask and the centroid
        plt.imshow(cell_mask)
        plt.plot(medial_axis_df.cropped_x, medial_axis_df.cropped_y, color='red')
        plt.plot(cropped_centroid[0], cropped_centroid[1], 'o')
        plt.show()
        # get the original centroid coordinates from the cropped centroid
        centroid = (cropped_centroid[0]+cell_pad[0], cropped_centroid[1]+cell_pad[1])
        
        return medial_axis_df, centroid, cropped_centroid
    
    
    def run_medial_axis(self):
        """
        This function gets the medial axis for all cells in the image,
        assembles the results in a dictionary, which is then saved in the specified folder.
        
        Exception
        ---------
        If the medial axis cannot be estimated, the cell is not included in the medial axis dictionary.
        """
        medial_axis_dict = {}
        for pos in self.mask_arrays:
            medial_axis_dict[pos]={}
            for cell in self.mask_arrays[pos][0]:
                cropped_cell_mask = self.mask_arrays[pos][0][cell]
                cell_pad = self.mask_arrays[pos][1][cell]
                try:
                    medial_axis_dict[pos][cell] = self.get_medial_axis(cropped_cell_mask, cell_pad)
                except:
                    plt.imshow(cropped_cell_mask)
                    plt.show()
                    print('This spec is probably not a cell')
            
        with open(self.save_path+'/'+self.experiment+'_medial_axis_dict', 'wb') as handle:
            pickle.dump(medial_axis_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
    

    # BACKGROUND ESTIMATION AND SUBTRACTION FUNCTIONS 
    def cell_free_bkg_estimation(self, masked_signal_image, step):
        """
        This function scans the image using squared regions of specified size (step) 
        and applies the average cell-free background fluorescence per region.
        This function is used in the self.back_sub() function.
        
        Parameters
        ----------
        masked_signal_image: 2D numpy array - the signal image were the cell pixels are annotated as 0 
                             and the non-cell pixels maintain their original grayscale values
        step: integer (should be a divisor or the square image dimensions) - the dimensions of the squared region where 
              the cell-free background fluorescence is averaged
                example: for an 2048x2048 image, 128 is a divisor and can be used as the size of the edge of the square 
    
        Returns
        -------
        A 2D numpy array where the cell-free average background is stored for each square region with specified step-size
        """
        zero_image = np.zeros(self.sensor) # initiated an empty image to store the average cell-free background
        
        for y in range(0, self.sensor[1], step):
            for x in range(0, self.sensor[0], step):
                # cropped_image = img_bkg_sig[y:(y+step), x:(x+step)]
                cropped_mask = masked_signal_image[y:(y+step), x:(x+step)]
#                mean_bkg = np.mean(cropped_mask[np.nonzero(cropped_mask)].ravel()) # get the mean of the non-zero pixels
#                mean_bkg = scipy.stats.mode(cropped_mask[cropped_mask!=0].ravel())[0][0] # get the mode of the non-zero pixels
                mean_bkg = np.median(cropped_mask[np.nonzero(cropped_mask)].ravel()) # get the mean of the non-zero pixels
                zero_image[y:(y+step), x:(x+step)] = mean_bkg # apply this mean fluorescence to the original empty image
                       
        return zero_image
    
    
    def back_sub(self, phase_image, signal_image, dilation, estimation_step, smoothing_sigma, show):
        """
        Local background estimation outside the cell regions (outside the otsu thresholded dilated cell area).
        The average background within a square of size equal to the 'estimation_step' is used to fill in the cellular background
        (within the dilated cell mask) and then a Gaussian smoothing is applied.
        
        Parameters
        ----------
        phsae_image: numpy.array - the phase contrast image
        signal_image: numpy.array - the image to be corrected
        dilation: non-negative integer - the number of dilation rounds for the cell mask
        estimation_step: positive_integer - the size of the square edge used for average background estimation
        smoothing_sigma: non-negative integer - the smoothing factor of the cell free background
        show: binary - True if the user wants to visualize the 2D surface fit
        
        Returns
        -------
        [0] The average background
        [1] The background corrected image (after subtracting the smoothed cell free signal)
        """
        print('Subtracting background...')
        # invert the image and apply an otsu threshold to separate the dimmest 
        # (or inversely brightest pixels) which correspond to the cells
        inverted_phase_image = 1/phase_image
        inverted_threshold = threshold_otsu(inverted_phase_image.ravel())
        phase_mask = inverted_phase_image > inverted_threshold
        # dilate the masked phase images
        threshold_masks_dil = ndimage.binary_dilation(phase_mask, iterations=dilation)
        threshold_masks_dil = np.array(threshold_masks_dil)
        # mask the signal image, excluding the dilated cell pixels
        masked_signal_image = signal_image * ~threshold_masks_dil
        if show == True:
            plt.figure(figsize=(10,10))
            plt.imshow(threshold_masks_dil)
            plt.show()
        # The dimensions of the averaging square
        step = estimation_step
        img_bkg_sig = self.cell_free_bkg_estimation(masked_signal_image, step)
        if show == True:
            plt.figure(figsize=(20,20))
            plt.imshow(img_bkg_sig, cmap='Blues')
            plt.clim(np.mean(img_bkg_sig.ravel())-5*np.std(img_bkg_sig.ravel()),np.mean(img_bkg_sig.ravel())+2.5*np.std(img_bkg_sig.ravel()))
            plt.colorbar()
            plt.show()
        # Smooth the reconstructed background image, with the filled cell pixels.
        img_bkg_sig = img_bkg_sig.astype(np.int16)
        img_bkg_sig = ndimage.gaussian_filter(img_bkg_sig, sigma=smoothing_sigma)
        norm_img_bkg_sig = img_bkg_sig/np.max(img_bkg_sig.ravel())
        if show == True:
            plt.figure(figsize=(20,20))
            plt.imshow(img_bkg_sig, cmap='Blues')
            # plt.clim(0,25*np.std(bkg_cor.ravel()))
            plt.colorbar()
            plt.show()
        # subtract the reconstructed background from the original signal image
        bkg_cor = (signal_image - img_bkg_sig)/norm_img_bkg_sig
        bkg_cor_2 = signal_image - img_bkg_sig
        # use this line if you want to convert negative pixels to zero
        # bkg_cor[bkg_cor<0]=0
        if show == True:
            plt.figure(figsize=(20,20))
            plt.imshow(bkg_cor, cmap='Blues')
            plt.clim(0,25*np.std(bkg_cor.ravel()))
            plt.colorbar()
            plt.show()
            plt.figure(figsize=(20,20))
            plt.imshow(img_bkg_sig*threshold_masks_dil, cmap='Blues')
            plt.clim(np.mean(img_bkg_sig.ravel())-5*np.std(img_bkg_sig.ravel()),np.mean(img_bkg_sig.ravel())+2.5*np.std(img_bkg_sig.ravel()))
            plt.colorbar()
            plt.show()
        
        return bkg_cor, np.mean(img_bkg_sig.ravel()), bkg_cor_2
    

    def get_background_corrected_images(self):
#        plt.imshow(snap.back_sub(phase_image, signal_image, 25, 128, 60, True)[2])
#        plt.colorbar()
        bkg_cor_arrays = {}
        fluor_channels = self.channels.copy()
        fluor_channels.remove('Trans')
        for ch in fluor_channels:
            bkg_cor_arrays[ch]={}
            for pos in self.image_arrays[ch].keys():
                print(self.experiment,', channel:',ch,', position:',pos, ', correcting background')
                phase_image = self.image_arrays['Trans'][pos][2]
                signal_image = self.image_arrays[ch][pos][2]
                bkg_cor_arrays[ch][pos] = self.back_sub(phase_image, signal_image, 25, 128, 60, False)[2]
        return bkg_cor_arrays
                
    
    def get_cell_mean_stats(self):
        cor_image_arrays = self.get_background_corrected_images()
        cell_list = []
        for pos in self.mask_arrays.keys():
            cell_list += list(self.mask_arrays[pos][0].keys())
        mean_df = pd.DataFrame()
        mean_df['cell_id'] = cell_list
        for ch in cor_image_arrays:
            area_dict  = {}
            mean_fluor_dict = {}
            std_fluor_dict = {}
            per_fluor_dict = {}
            total_fluor_dict = {}
            position_dict = {}
            for pos in cor_image_arrays[ch]:
                for cl in self.mask_arrays[pos][0]:
                    cell_mask = self.mask_arrays[pos][0][cl]
                    cell_pad = self.mask_arrays[pos][1][cl]
                    crop_signal_image =  cor_image_arrays[ch][pos][cell_pad[1]:cell_pad[3], cell_pad[0]:cell_pad[2]]
#                    plt.imshow(crop_signal_image)
#                    plt.show()
#                    plt.imshow(cell_mask)
#                    plt.show()
#                    input()
                    if 'cell_area_px' not in mean_df:
                        area_dict[cl] = np.nonzero(cell_mask)[0].shape[0]
                    mean_fluor_dict[cl] = crop_signal_image[np.nonzero(cell_mask)].mean()
                    total_fluor_dict[cl] = crop_signal_image[np.nonzero(cell_mask)].sum()
                    std_fluor_dict[cl] = crop_signal_image[np.nonzero(cell_mask)].std()
                    per_fluor_dict[cl] = np.percentile(crop_signal_image[np.nonzero(cell_mask)], 90)
                    position_dict[cl] = pos
            mean_df[ch+'_mean'] = mean_df.cell_id.map(mean_fluor_dict)
            mean_df[ch+'_total'] = mean_df.cell_id.map(total_fluor_dict)
            mean_df[ch+'_std'] = mean_df.cell_id.map(std_fluor_dict)
            mean_df[ch+'_90'] = mean_df.cell_id.map(per_fluor_dict)
            mean_df['experiment'] = self.experiment
            mean_df['position'] = mean_df.cell_id.map(position_dict)
            if 'cell_area_px' not in mean_df:
                mean_df['cell_area_px'] = mean_df.cell_id.map(area_dict)
        mean_df.to_pickle(self.save_path+'/'+self.experiment+'_mean_df', compression='zip')
        return mean_df
    
    
    def get_oned_coordinates(self, cell_mask, medial_axis_df, half_window): 

        cell_mask_df = pd.DataFrame()
        cell_mask_df['x'] = np.nonzero(cell_mask)[1]
        cell_mask_df['y'] = np.nonzero(cell_mask)[0]
    #    cell_mask_df['z'] = fluor_image[np.nonzero(cell_mask)]
    
        def get_pixel_projection(pixel_x, pixel_y, medial_axis_df, half_window):
            
            medial_axis_df['pixel_distance'] = np.sqrt((medial_axis_df.cropped_x-pixel_x)**2+(medial_axis_df.cropped_y-pixel_y)**2)
            min_df = medial_axis_df[medial_axis_df.pixel_distance == medial_axis_df.pixel_distance.min()]
            min_arch_centered_length = min_df.arch_length_centered.values[0]
            min_arch_scaled_length =  min_df.arch_length_scaled.values[0]
            min_distance_abs = min_df.pixel_distance.values[0]
            min_index = min_df.index.values[0]
            medial_axis_coords = (min_df.cropped_x.values[0], min_df.cropped_y.values[0])
            
            def get_relative_distance(min_distance_abs, medial_axis_df, min_index, medial_axis_coords, pixel_x, pixel_y, half_window):
        
                if min_index>=half_window and min_index<medial_axis_df.index.max()-half_window:
                    index_range = (min_index-half_window, min_index+half_window)
                elif min_index<half_window and min_index<medial_axis_df.index.max()-half_window:
                    index_range = (0, min_index+half_window)
                elif min_index>=half_window and min_index>=medial_axis_df.index.max()-half_window:
                    index_range = (min_index-half_window, medial_axis_df.index.max())
                
                delta_x = (medial_axis_df.iloc[index_range[1]].cropped_x -  medial_axis_df.iloc[index_range[0]].cropped_x)
                delta_y = (medial_axis_df.iloc[index_range[1]].cropped_y -  medial_axis_df.iloc[index_range[0]].cropped_y)
                medial_axis_vector = [delta_x, delta_y]
                
                delta_x = pixel_x - medial_axis_coords[0]
                delta_y = pixel_y - medial_axis_coords[1]
                pixel_vector = [delta_x, delta_y]
                
                cross_product = np.cross(medial_axis_vector, pixel_vector)
                if cross_product != 0:
                    min_distance = np.sign(cross_product)*min_distance_abs
    #                return min_distance
                elif cross_product == 0:
    #                half_window+=1
    #                get_relative_distance(min_distance_abs, medial_axis_df, min_index, medial_axis_coords, pixel_x, pixel_y, half_window)
                    min_distance = 0
                return min_distance
            
            min_distance = get_relative_distance(min_distance_abs, medial_axis_df, min_index, medial_axis_coords, pixel_x, pixel_y, half_window)
        
            return min_arch_centered_length, min_arch_scaled_length, min_distance
            
                
        cell_mask_df['oned_coords'] = cell_mask_df.apply(lambda x: get_pixel_projection(x.x, x.y, medial_axis_df, half_window=5), axis=1)
        cell_mask_df[['arch_length', 'scaled_length', 'width']] = pd.DataFrame(cell_mask_df.oned_coords.to_list(), index=cell_mask_df.index)
        cell_mask_df = cell_mask_df.drop(['oned_coords'], axis=1)
        
        return cell_mask_df
        
    
    def apply_oned_coordinates(self):
        
        
        with open(self.save_path+'/'+self.experiment+'_medial_axis_dict', 'rb') as handle:
            medial_axis_dict = pickle.load(handle)
        
        oned_coords_dict = {}
        cell_length_dict = {}
        
        cor_image_arrays = self.get_background_corrected_images()
        
        for pos in self.mask_arrays:
            for cl in self.mask_arrays[pos][0]:
                if cl in medial_axis_dict[pos]:
                    print(cl, 'mapping 1D pixel coordinates')
                    cell_mask = self.mask_arrays[pos][0][cl]
                    cell_pad = self.mask_arrays[pos][1][cl]
                    medial_axis_df = medial_axis_dict[pos][cl][0]
                    cell_mask_df = self.get_oned_coordinates(cell_mask, medial_axis_df, half_window=10)
                    for ch in cor_image_arrays:
                        crop_signal_image =  cor_image_arrays[ch][pos][cell_pad[1]:cell_pad[3], cell_pad[0]:cell_pad[2]]
                        cell_mask_df[ch+'_fluor'] = crop_signal_image[np.nonzero(cell_mask)]
                    oned_coords_dict[cl] = cell_mask_df
                    cell_length_dict[cl] = medial_axis_df.arch_length_centered.max()*2

        pickle.dump(oned_coords_dict, open(self.save_path+'/'+self.experiment+'_oned_coords_dict', 'wb'))
        pickle.dump(cell_length_dict, open(self.save_path+'/'+self.experiment+'_cell_length_dict', 'wb'))
        
        return oned_coords_dict
    
    
    
    
class particle_positions_snapshots(unet_snapshots):
    """
    Developer: Alexandros Papagiannakis, Christine Jacobs-Wagner lab, Stanford University, 2020
    
    This class contains all the functions to detect the positions and centers 
    of difraction limited spots (e.g. GFP-Î¼NS particles).
    
    It inherits attributes and variables from the unet_snapshots class above.
    
    Functions included in this class:
        __init__
            inherits from the unet_snapshots class
        fitgaussian
            moments
        log_adaptive_filter
        log_filter
        particle_segmentation
            post_processing
        check_particle_segmentation_parameters
        show_gaussian_particle_fit
        estimate_particle_center
        get_particle_fluorescence
        assign_particles_to_cells
        get_cell_with_particle
        get_oneD_projection(
            get_relative_distance
        apply_particle_projection
        getting_the_particles_in_snapshots
        
    """
    def __init__(self, unet_path, snapshots_path, experiment, save_path):
        super().__init__(unet_path, snapshots_path, experiment, save_path)
        
    
    def gaussian(self, height, center_x, center_y, width_x, width_y, rotation):
        """
        Returns a gaussian function with the given parameters
        
        Reference
        ---------
        Originally implemented by Andrew Giessel
        Code available on GitHub:
            https://gist.github.com/andrewgiessel/6122739
        
        Notes
        -----
        Modified by Alexandros Papagiannakis to correct the rotation of the 2D Gaussian around the central pixel
        """
        width_x = float(width_x)
        width_y = float(width_y)
        
        rotation = np.deg2rad(rotation)
    #    center_x = center_x * np.cos(rotation) - center_y * np.sin(rotation)
    #    center_y = center_x * np.sin(rotation) + center_y * np.cos(rotation)
        
        def rotgauss(x,y):
            x = x-center_x # modification
            y = y-center_y # modification
            xp = x * np.cos(rotation) - y * np.sin(rotation)
            yp = x * np.sin(rotation) + y * np.cos(rotation)
            xp = xp + center_x
            yp = yp + center_y
            g = height*np.exp(
                -(((center_x-xp)/width_x)**2+
                  ((center_y-yp)/width_y)**2)/2.)
            return g
        return rotgauss
    
    def fitgaussian(self, data):
        """Returns (height, x, y, width_x, width_y)
        the gaussian parameters of a 2D distribution found by a fit"""
        
        def moments(data):
            """Returns (height, x, y, width_x, width_y)
            the gaussian parameters of a 2D distribution by calculating its
            moments """
            total = data.sum()
            X, Y = np.indices(data.shape)
            x = (X*data).sum()/total
            y = (Y*data).sum()/total

            col = data[:, int(y)]

            width_x = np.sqrt(abs((np.arange(col.size)-y)**2*col).sum()/col.sum())

            row = data[int(x), :]

            width_y = np.sqrt(abs((np.arange(row.size)-x)**2*row).sum()/row.sum())
            height = data.max()
            return height, x, y, width_x, width_y, 0.0
        
        params = moments(data)
        errorfunction = lambda p: np.ravel(self.gaussian(*p)(*np.indices(data.shape)) - data)
        p, success = scipy.optimize.leastsq(errorfunction, params)
        return p
    

    
    #--------- IMAGE FILTERS ---------#
    # Customized image filters used to sharpen or smooth, as well as threshold images.
    def log_adaptive_filter(self, image, parameters): 
        """
        This fucntion constructs an LoG filer (Laplace of Gaussian) as well as an adaptive filter to segment particles
        
        Parameters
        ---------
        image: numpy array - the image to be filtered and thresholded (usually this is the background subtracted image)
        particle_detection_parameters: list - the parameters for the LoG/adaptive filter
            [0] Smoothing factor for the Gaussian filter (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian)
            [1] Laplace threshold (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.laplace)
            [2] Hard threshold on the LoG image as a percentile of the brightest pixels
            [3] Gaussian smoothing factor before applying the adaptive threshold (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian)
            [4] Block_size for the adaptive threshold (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.threshold_local)
            [5] Offset for the adaptive threshold (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.threshold_local)
            [6] Mask erosion applied after the adaptive threshold (it can be set to zero). Otherwise it is a positive integer (erosion rounds).
    
        Returns
        -------
        [0] The thresholded image after the LoG filter
        [1] The image after the adaptive threshold
        [2] The filter after multiplying the thresholded smoothed image with the adaptively thresolded image
        """
        
        # image = particle.back_sub(particle.particle_images[0], show=True)[0]
        # parameters = [4, 1000, 99.5] for testing
        image[image<0]=0
        # LoG filter with hard threshold
        # image_smoothed = filters.gaussian(image, parameters[0])
        image_smoothed = ndimage.gaussian_filter(image, sigma = parameters[0])
        image_laplace = filters.laplace(image_smoothed, parameters[1])
        image_pixel_intensities = image_laplace.ravel()
        sorted_pixel_intensities = np.sort(image_pixel_intensities)
        pixel_intensity_threshold = sorted_pixel_intensities[int((parameters[2]/100)*len(sorted_pixel_intensities))]
        log_image = image_laplace > pixel_intensity_threshold
        # plt.figure(figsize=(20,20))
        # plt.imshow(masked_image)
        # # plt.colorbar()
        # plt.show()
        # Adaptice threshold
        # image_smoothed_2 = filters.gaussian(image, parameters[3])
        image_smoothed_2 = ndimage.gaussian_filter(image, sigma=parameters[3])
        adaptive_threshold = threshold_local(image_smoothed_2, block_size=parameters[4], offset=parameters[5])
        # adaptive_threshold = threshold_local(image, block_size=parameters[4], offset=parameters[5])
        adaptively_masked_image = image_smoothed_2 > adaptive_threshold
        # plt.figure(figsize=(20,20))
        # plt.imshow(adaptively_masked_image)
        # # plt.colorbar()
        # plt.show()
        masked_image = adaptively_masked_image * log_image
            # erode the image if the erosion iterations are higher than 1
        if parameters[6] > 0:
            adaptively_masked_image = scipy.ndimage.morphology.binary_erosion(masked_image, iterations = parameters[6])
            final_image = adaptively_masked_image
        elif parameters[6] == 0:
            final_image = masked_image

        return log_image, adaptively_masked_image, final_image
    

    def log_filter(self, image, parameters): 
        """
        This fucntion constructs an LoG filer (Laplace of Gaussian) as well as an adaptive filter to segment particles
        
        Parameters
        ----------
        image: numpy array - the image to be filtered and thresholded (usually this is the background subtracted image)
        particle_detection_parameters: list - the parameters for the LoG/adaptive filter
            [0] Smoothing factor for the Gaussian filter (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian)
            [1] Laplace threshold (https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.laplace)
            [2] Hard threshold on the LoG image as a percentile of the brightest pixels
        
        Returns
        -------
        The thresholded image after the LoG filter
        """
        
        # image = particle.back_sub(particle.particle_images[0], show=True)[0]
        # parameters = [4, 1000, 99.5] for testing
        image[image<0]=0
        # LoG filter with hard threshold
        # image_smoothed = filters.gaussian(image, parameters[0])
        image_smoothed = ndimage.gaussian_filter(image, sigma = parameters[0])
        image_laplace = filters.laplace(image_smoothed, parameters[1])
        image_pixel_intensities = image_laplace.ravel()
        sorted_pixel_intensities = np.sort(image_pixel_intensities)
        pixel_intensity_threshold = sorted_pixel_intensities[int((parameters[2]/100)*len(sorted_pixel_intensities))]
        masked_image = image_laplace > pixel_intensity_threshold
        return masked_image

    
    #---------- SEGMENTATION AND TRACKING ALGORITHMS -----------#
    # These functions are used to segment and track the particles

    def particle_segmentation(self, back_sub_image, log_adaptive_parameters, min_spot_size, max_spot_size, min_spot_aspect_ratio, post_processing_threshold):
        """
        This function is used to segment particle in the fluorescence image.
        It first applies an log_adaptive filer (see line 883) to filter and segment the image.
        If the detected spots are larger than the max_spot_size parameter, or have a very low aspect ratio (minor axis / major axis) then the algorithm zooms into
        the spot and applies new segmentation parameters. This happens since very large or elongated spots are probably attributed to many particles recognized as one.
        By applying stricter parameters it is possible to separate the clustered particles. 
        
        Parameters
        ----------
        back_sub_image: 2D numpy ndarray: the particle fluorescence image to be segmented (background corrected)
        log_adaptive_parameters: list - the parameters for segmentation (log_adaptive filter parameters)
        min_spot_size: integer - the expected min size of the particle in pixels
        max_spot_size: integer - the expected max size of the particle in pixels
        min_spot_aspect_ratio: float - the expected minimum aspect ration (minor/major axis) of thge particle
        post_processing_threshold: float or integer - The hard threshold for segmented the clustered particles as a percentage of brightest pixels
    
        Returns
        -------
        A pandas dataframe:
            columns:
            'centroid' microtubule_centroid -> the centroid coordinates of the particle (x,y tuple)
            'minor_axis' microtubule_minor_axis -> the minor axis of the segmemted particle spot in px
            'major_axis' microtubule_major_axis -> the major axis of the segmented particle spot in px
            'aspect_ratio' particle_aspect_ratio_list -> the minor/major axis ratio
            'area' microtubule_area -> the area of the segmented particle spot in px
            'experiment' self.experiment -> the experiment (inherited from the class)
        """
        def post_processing(back_sub_image, particle_centroid_yx, particle_major_axis, particle_minor_axis, post_processing_threshold, particle_lists):
            """
            This function is used to reprocess the particle ROI in otder to separate clustered particles.
            It zooms into the segmented spot and applies a new LoG filter to separate multiple particles that
            were segmented as one (very proximal to each other).
            This function is applied if the segmented spot is larger than the maximum spot area selected by the user
            or if the aspect ratio (minor/major axis) of the spot is lower than expected.
            
            If the pos-processed regoins of interest result in smaller or rounder segmented spots within the specified ranges of 
            particle areas and aspect ratios, then the particle coordinates and dimenions lists are updated including these
            post-processed particles.
            
            Parameters
            ----------
            back_sub_image: a 2D numpy ndarray of the backgrund subtracted image
            particle_centroid_yx: tuple - the (y,x) coordinates of the segmented spot
            particle_major_axis: float - the major axis of the segmented spot
            particle_minor_axis: float - the minor axis of the segmented spot
            post_processing_threshold: float or integer - The hard threshold for segmented the clustered particles as a percentage of brightest pixels
            particle_lists: a list containing the particle_centroid, particle_minor_axis, particle_major_axis, particle_aspect_ratio and particle_area lists.
                [particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list]
            Returns
            -------
            The updated particle_centroid [0], particle_minor_axis [1], particle_major_axis [2], particle_aspect_ratio [3] and particle_area lists [4].
            """
            particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list = particle_lists
            
            if type(post_processing_threshold) == int or type(post_processing_threshold) == float:
                if int(particle_centroid_yx[0]-particle_major_axis/2) >= 0 and int(particle_centroid_yx[0]+particle_major_axis/2) < self.sensor[1] and int(particle_centroid_yx[1]-particle_major_axis/2) >= 0 and int(particle_centroid_yx[1]+particle_major_axis/2) < self.sensor[0]:
                    print(particle_centroid_yx[::-1], particle_area, particle_major_axis, particle_minor_axis,'post processing...')
                    # crop the image around the spot position - zooming into the spot of interest and apply a stricter log filter
                    cropped_image = back_sub_image[int(particle_centroid_yx[0]-particle_major_axis/2):int(particle_centroid_yx[0]+particle_major_axis/2), int(particle_centroid_yx[1]-particle_major_axis/2):int(particle_centroid_yx[1]+particle_major_axis/2)]
                    fluorescence_threshold = np.sort(cropped_image.ravel())[-int(len(cropped_image.ravel())*((100-post_processing_threshold)/100))]
                    
                    cropped_filtered_image = cropped_image > fluorescence_threshold
                    # emumerate the segmented spots in the cropped image
                    sub_particle_labels = label(cropped_filtered_image)
                    # get the features of the segmented spots
                    for sub_particle_region in regionprops(sub_particle_labels):
                        sub_particle_centroid_yx = sub_particle_region.centroid
                        sub_particle_centroid_yx_corrected = (sub_particle_centroid_yx[0] + int(particle_centroid_yx[0]-particle_major_axis/2), sub_particle_centroid_yx[1] + int(particle_centroid_yx[1]-particle_major_axis/2))
                        sub_particle_minor_axis = sub_particle_region.minor_axis_length
                        sub_particle_major_axis = sub_particle_region.major_axis_length
                        sub_particle_area = sub_particle_region.area
                        
                        if sub_particle_area >= min_spot_size  and sub_particle_minor_axis > 0 and sub_particle_major_axis > 0:
                            sub_particle_aspect_ratio = sub_particle_minor_axis / sub_particle_major_axis

                            particle_centroid_list.append(sub_particle_centroid_yx_corrected[::-1])
                            particle_minor_axis_list.append(sub_particle_minor_axis)
                            particle_major_axis_list.append(sub_particle_major_axis)
                            particle_aspect_ratio_list.append(sub_particle_aspect_ratio)
                            particle_area_list.append(sub_particle_area)
            # Only if the post-processing threshold is a % float or integer of brightest pixels it is then used to split clustered spots within the region
            elif post_processing_threshold == None:
                print(particle_centroid_yx[::-1], particle_area, particle_major_axis, particle_minor_axis, 'the post-processing parameters were not defined. post-processing aborted...')
            
            return particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list

        print('Filtering image...')
        filtered_image = self.log_adaptive_filter(back_sub_image, log_adaptive_parameters)[2]
        # ennumerate the separated masks in the image using the labels functioon from the skimage.measure library
        particle_labels = label(filtered_image)
        # These lists will store the particle data if the particles satisfy all the conditions
        particle_centroid_list = []
        particle_minor_axis_list = []
        particle_major_axis_list = []
        particle_aspect_ratio_list = []
        particle_area_list = []
    
        for particle_label in regionprops(particle_labels):
            particle_centroid_yx = particle_label.centroid
            particle_minor_axis = particle_label.minor_axis_length
            particle_major_axis = particle_label.major_axis_length
            particle_area = particle_label.area
            # This condition is important to avoid numerical erros (very small particles of 1 or 2 pixels in size) and avoids division by zero
            if (particle_area > 0 and particle_minor_axis > 0 and particle_major_axis > 0):
                particle_aspect_ratio = particle_minor_axis/particle_major_axis
                if particle_area >= min_spot_size and particle_area <= max_spot_size and particle_aspect_ratio >= min_spot_aspect_ratio:
                    # reversing the centroid ro correspond to the xy and not the yx coordinates
                    particle_centroid_list.append(particle_centroid_yx[::-1])
                    particle_minor_axis_list.append(particle_minor_axis)
                    particle_major_axis_list.append(particle_major_axis)
                    particle_aspect_ratio_list.append(particle_aspect_ratio)
                    particle_area_list.append(particle_area)
                elif particle_area >= min_spot_size and particle_area <= max_spot_size and particle_aspect_ratio < min_spot_aspect_ratio:
                    print(particle_area, particle_minor_axis, particle_major_axis)
                    particle_lists = [particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list]
                    particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list = post_processing(back_sub_image, particle_centroid_yx, particle_major_axis, particle_minor_axis, post_processing_threshold, particle_lists)
                elif (particle_area > max_spot_size):
                    print(particle_area, particle_minor_axis, particle_major_axis)
                    particle_lists = [particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list]
                    particle_centroid_list, particle_minor_axis_list, particle_major_axis_list, particle_aspect_ratio_list, particle_area_list = post_processing(back_sub_image, particle_centroid_yx, particle_major_axis, particle_minor_axis, post_processing_threshold, particle_lists)
            elif (particle_area <= 0 or particle_minor_axis <= 0 or particle_major_axis <= 0):
                print(particle_centroid_yx[::-1], particle_area, particle_major_axis, particle_minor_axis, 'Error: division by zero, spot aborted')
        # create a pandas dataframe with the segmented particle coordinates and statistics per frame
        particle_segmentation_df = pd.DataFrame()
        particle_segmentation_df['centroid'] = particle_centroid_list
        particle_segmentation_df['minor_axis'] = particle_minor_axis_list
        particle_segmentation_df['major_axis'] = particle_major_axis_list 
        particle_segmentation_df['area'] = particle_area_list
        particle_segmentation_df['aspect_ratio'] = particle_aspect_ratio_list
        particle_segmentation_df['experiment'] = self.experiment
        
        return particle_segmentation_df
    
    
    def check_particle_segmentation_parameters(self, channel, position, post_processing=False):
        """
        This code is used to check the segmentation parametetrs.
        The image uneven background is subtracted. 
        Then the user is asked to input the particle segmentation parameters. Default values are recommended.
        All the segmentation parameters are used in the self.particle_segmentation function (line 933)
        
        The segmentation is shown in the particle fluorescence channel.
        
        Parameters
        ----------
        frame: non-negative integer - the frame in the fast time-lapse to be checked
        post_processing: binary - if True the user is asked to provide post-processing parameters

        Returns
        -------
        [0] log_adaptive_parameters
        [1] the maximum particle area
        [2] the minimum particle aspect ratio
        [3] log_adaptive post_processing parameters (if post_processing is False an empty list is returned.
        Providing an empty list in the self.particle_segmentation function will abort post-processing)
        """
        # good default parameters
#        ([3.0, 1000, 95, 1, 9, -5.0, 0], 3, 90, 0.3, [])
        
        
         
        particle_images = self.image_arrays[channel]
        phase_images = self.image_arrays['Trans']
        
        background_subtraction = self.back_sub(phase_images[position][2], particle_images[position][2], 25, 128, 60, False)
        back_sub_image = background_subtraction[0]

        i = 0
        while i == 0:
            try:
                gaussian_param = float(input('Choose the gaussian smoothing factor (recommended: 4):'))
            except ValueError:
                print('please choose a number')
                gaussian_param = float(input('Choose the gaussian smoothing factor (recommended: 4):'))
            
            try:
                laplace_param = int(input('Choose the laplace filter factor (recommended: 1000):'))
            except ValueError:
                print('please choose a number')
                laplace_param = int(input('Choose the laplace filter factor (recommended: 1000):'))
            
            try:
                log_threshold = float(input('Choose the hard threshold of the LoG filter (recommended: 97):'))
            except ValueError:
                print('please choose a number')
                log_threshold = float(input('Choose the gaussian smoothing factor (recommended: 97):'))
            
            try:
                adaptive_smoothing = float(input('Choose the gaussian smoothing factor before the adaptive thresholding (recommended: 2):'))
            except ValueError:
                print('please choose a number')
                adaptive_smoothing = float(input('Choose the gaussian smoothing factor before the adaptive thresholding (recommended: 2):'))
            
            try:
                block_size_in = int(input('Choose the block size for the adaptive smoothing (recommended: 9, odd number):'))
            except ValueError:
                print('please choose an odd integer positive number')
                block_size_in = int(input('Choose the block size for the adaptive smoothing (recommended: 9, odd number):'))
            
            try:
                offset_in = float(input('Choose the offset for the adaptive smoothing (recommended: -2):'))
            except ValueError:
                print('please choose a number')
                offset_in = float(input('Choose the offset for the adaptive smoothing (recommended: -2):'))
            
            try:
                erosion_in = int(input('Choose the number of erosion rounds for the adaptively thresholded mask (reccomdned: 0):'))
            except ValueError:
                print('please choose a non-negative integer')
                erosion_in = int(input('Choose the number of erosion rounds for the adaptively thresholded mask (reccomdned: 0):'))
            
            try:
                min_particle_size = int(input('Choose the expected minimum size of the particle (recommended: 3):'))
            except ValueError:
                print('please choose a number')
                min_particle_size = int(input('Choose the expected minimum size of the particle (recommended: 3):'))
            
            try:
                max_particle_size = int(input('Choose the expected maximum size of the particle (recommended: 150):'))
            except ValueError:
                print('please choose a number')
                max_particle_size = int(input('Choose the expected maximum size of the particle (recommended: 150):'))
            
            try:
                min_particle_aspect_ratio = float(input('Choose the expected minimum aspect ratio of the aprticle (recommended: 0.3):'))
            except ValueError:
                print('please choose a number')
                min_particle_aspect_ratio = float(input('Choose the expected minimum aspect ratio of the aprticle (recommended: 0.3):'))
            
            if post_processing == True:
                try:
                    post_processing_threshold = float(input('Choose the post processing threshold (recommended: 90):'))
                except ValueError:
                    print('please choose a number')
                    post_processing_threshold = float(input('Choose the post processing threshold (recommended: 90):'))
            elif post_processing ==False:
                post_processing_threshold = None
                

            log_adaptive_parameters = [gaussian_param, laplace_param, log_threshold, adaptive_smoothing, block_size_in, offset_in, erosion_in]
            particle_segmentation_df = self.particle_segmentation(back_sub_image, log_adaptive_parameters, min_particle_size, max_particle_size, min_particle_aspect_ratio, post_processing_threshold)
            filtered_image = self.log_adaptive_filter(back_sub_image, log_adaptive_parameters)
            
            print('The LoG image')
            plt.figure(figsize=(20,20))
            plt.imshow(filtered_image[0])
            plt.show()
            
            print('Press enter to continue')
            input()
            
            print('The adaptively thresholded image')
            plt.figure(figsize=(20,20))
            plt.imshow(filtered_image[1])
            plt.show()
            
            print('Press enter to continue')
            input()
            
            print('Combining the LoG hard thresholded and the adaptively thresholded images')
            plt.figure(figsize=(20,20))
            plt.imshow(filtered_image[2])
            plt.show()
            
            print('Press enter to continue')
            input()
            
            # Print a figure with all the segmented particles and get the particle centers
            print('showing segmentation...')
            fig, ax = plt.subplots(figsize=(20, 20))
            # The vmin and vmax values can be adjusted to change the LUTs, as well as the colormap
            ax.imshow(back_sub_image, cmap='viridis', vmin=0, vmax=300)
            
            for index, row in particle_segmentation_df.iterrows():
                rect = matplotlib.patches.Rectangle((row['centroid'][0]-row['major_axis']/2,row['centroid'][1]-row['major_axis']/2), row['major_axis'], row['major_axis'], fill=False, edgecolor='coral', linewidth=1)
                ax.add_patch(rect)
                ax.set_axis_off()
                plt.tight_layout()
            
            plt.show()
            
            j = 0
            while j == 0:
                decision = str(input('if the parameters are good choose "g", esle type "b":'))
                decision = decision.lower()
                if decision == 'g':
                    i += 1
                    j += 1
                elif decision == 'b':
                    j += 1
                else:
                    print('wrong input, please try again...')
        
        return log_adaptive_parameters, min_particle_size, max_particle_size, min_particle_aspect_ratio, post_processing_threshold

    
    def show_gaussian_particle_fit(self, cropped_particle_image, fitted_subpx, fitted_px,  param):
        """
        This function plots the particle fluorescence 2D gaussian fit and the particle center estimation.
        
        Parameters
        ----------
        cropped_particle_image: 2D numpy array - the cropped particle image (within the bounding box)
        fitted_subpx: 2D numpy array - the blurred particle image
        fitted_px: 2D numpy array - the raw particle image
        param: height, y, x, width_y, width_x, rotation - parameters of the 2D Gaussian fit
        """
        (height, y, x, width_y, width_x, rotation) = param
        # PLOT 1
        fig, ax = plt.subplots(figsize=(10, 5))
        plt.imshow(fitted_subpx)
        ax = plt.gca()
        plt.text(0.95, 0.05, """
                 amp: %.1f
                 x : %.1f
                 y : %.1f
                 width_x : %.1f
                 width_y : %.1f""" %(height, x, y, width_x, width_y),
                 fontsize=16, horizontalalignment='right',
                 verticalalignment='bottom', transform=ax.transAxes)
        plt.show()
        # PLOT 2
        # Fit a citcle at the center of each particle, which corresponds to the peak of the Gaussian
        circle = matplotlib.patches.Circle((x, y), radius=0.1, color='red', fill=False, linewidth=3)
        fig, ax = plt.subplots(figsize=(10, 5))
        plt.imshow(cropped_particle_image)
        plt.contour(fitted_px, cmap=plt.cm.Greys)
        ax.add_patch(circle)
        print('2D gaussian params:',param)
        plt.show()  
    

    def estimate_particle_center(self, bkg_cor_image, particle_center, box_size, gaussian_fit_show=False):
        """
        This function fits the 2D Gaussian to estimate the particle center.
        
        Parameters
        ----------
        bkg_cor_image: 2D numpy array - the background corrected particle fluorescence image
        particle_center: tuple of floats (x,y) corresponding to the center of mass of the particle label
        box_size: odd integer - the size of the box that is used to fit the 2D gaussian for the particle center estimation (5 or 11 suggested)
        gaussian_fit_show: bool - True to show the 2D Gaussian fit
                                  False otherwise
        Returns
        -------
        gaussian_particle_center: tupel of (x,y) floats: the estimate particle center (subpixel)
        brightest_raw_pixels: numpy array - the values of the 60% brightest raw pixels
        brightest_fitted_pixels: numpy array - the values of the 60% brightest smoothed pixels
            returns 'none' if one of the following exceptions is met.
        
        Exception
        ---------
        If the approximated particle center falls outside the bounding box, 
                the spot is not diffratcion limited and the particle is aborted.
                        'none' is returned
        If the particle bounding box falls outside the sensor dimensions, 
            'none' is returned
        If the particle eccentricity is below 0.1 or above 19,
            This spot probably corresponds to multiple clustered particles.
                'none' is returned
        """
        half_side = int((box_size-1)/2)
        cropped_particle_image = bkg_cor_image[(int(particle_center[1])-half_side):(int(particle_center[1])+(half_side+1)), (int(particle_center[0])-half_side):(int(particle_center[0])+(half_side+1))]
        if (cropped_particle_image.shape[0] > 0 and cropped_particle_image.shape[1] > 0):
            # create a meshgrid with 0.1 pixel resolution
            Xin, Yin = np.mgrid[0:cropped_particle_image.shape[1]:0.1,0:cropped_particle_image.shape[0]:0.1]
            # create a meshgrid with single pixel resolution
            Xin2, Yin2 = np.mgrid[0:cropped_particle_image.shape[1]:1,0:cropped_particle_image.shape[0]:1]
            # fit a 2D gaussian with rotation to the cropped image
            try:
                param = self.fitgaussian(cropped_particle_image)
                (height, y, x, width_y, width_x, rotation) = param
                # set an eccentricity threshold and the center coordinates fall within the box
                if (width_y/width_x > 0.1 and width_y/width_x < 10 and x<box_size and y<box_size and x>0 and y>0):
                    gaussian_fit = self.gaussian(*param)
                    # fit a guassian to a lattice of Xin by Yin pixels
                    fitted_subpx = gaussian_fit(Xin, Yin)
                    fitted_px = gaussian_fit(Xin2, Yin2)
                    # Get the 60% brightest smoothed pixels
                    brightest_fitted_pixels = fitted_px[fitted_px>np.percentile(fitted_px,40)]
                    # Get the 60% brightest raw pixels
                    brightest_raw_pixels = cropped_particle_image[cropped_particle_image>np.percentile(cropped_particle_image, 40)]
                    # Correct the gaussian coordinates to the original image dimensions (from the cropped dimensions)
                    gaussian_x_coord = x + int(particle_center[0])-half_side
                    gaussian_y_coord = y + int(particle_center[1])-half_side
                    gaussian_particle_center = (gaussian_x_coord, gaussian_y_coord)
                    if gaussian_fit_show == True:
                        self.show_gaussian_particle_fit(cropped_particle_image, fitted_subpx, fitted_px,  param)
            
                    return gaussian_particle_center, brightest_raw_pixels, brightest_fitted_pixels, param
                else:
                    print('This particle did not pass the eccentricity and position conditions. Particle position aborted:', particle_center)
                    return 'none'
            except IndexError:
                print('The approximate particle center is outside the square bounds. Particle position aborted:', particle_center)
                return 'none'
        else:
            print('This particle position spec ranges out of bounds. Particle position aborted:', particle_center)
            return 'none'

    
    def get_particle_fluorescence(self, metric, operation, brightest_raw_pixels, brightest_fitted_pixels, gaussian_vol):
        """
        This function is used to estimate the particle fluorescence at each position
        
        Paramters
        ---------
        metric: string, the column of the pandas dataframe which is used as a proxy for particle size. 
                Choose 'gaussian volume', 'raw pixels' or 'smoothed pixels'
        operation: function, the mathematical operation applied to the bin_column as a particle size proxy. 
                    Choose 'mean', 'median', 'sum' or 'max'
        brightest_raw_pixels: numpy array - the 60% brightest raw pixels 
        brightest_fitted_pixels: numpy array - the 60% brightest smoothed pixels 
        gaussian_vol: the volume of the fitted 2D Gaussian to the particle pixels
        
        Returns
        -------
        particle_vol: float - the particle fluorescence value corresponding to the chosen metric and operation
        
        Raises
        ------
        ValueError if an invalid metric or operation are included as inputs
        """
        # estimate the particle volume statistic based on a given metric and operation
        if metric == 'gaussian volume':
            particle_vol = gaussian_vol.copy()
            operation = 'none'
        elif metric == 'raw pixels':
            if operation == 'mean':
                particle_vol = np.mean(brightest_raw_pixels)
            elif operation == 'median':
                particle_vol = np.median(brightest_raw_pixels)
            elif operation == 'sum':
                particle_vol = np.sum(brightest_raw_pixels)
            elif operation == 'max':
                particle_vol = np.max(brightest_raw_pixels)
            else:
                raise ValueError("Choose 'mean', 'median', 'sum' or 'max' as the operation.")
        elif metric == 'smoothed pixels':
            if operation == 'mean':
                particle_vol = np.mean(brightest_fitted_pixels)
            elif operation == 'median':
                particle_vol = np.median(brightest_fitted_pixels)
            elif operation == 'sum':
                particle_vol = np.sum(brightest_fitted_pixels)
            elif operation == 'max':
                particle_vol = np.max(brightest_fitted_pixels)
            else:
                print("wrong operation for particle fluorescence estimation")
                raise ValueError("Choose 'mean', 'median', 'sum' or 'max' as the operation.")
        else:
            print("wrong metric for particle fluorescence estimation")
            raise ValueError("Choose 'gaussian volume', 'raw pixels', or 'smoothed pixels' as a metric")
        
        return particle_vol
        

    def assign_particles_to_cells(self, particle_position, cell_labels):
        
        particle_x = int(round(particle_position[0], 0))
        particle_y = int(round(particle_position[1], 0))
        
        if particle_x < 2048 and particle_y < 2048:
            return cell_labels[particle_y, particle_x]
        else:
            return 0
    
    
    def get_cell_with_particle(self, particle_position, position, position_string):
    
        mask_labels = self.get_unet_mask(position)
        
        cell_label = self.assign_particles_to_cells(particle_position, mask_labels)
        
        if cell_label>0:
            cell_id = self.get_cell_id_from_label(cell_label, position_string)
        elif cell_label==0:
            cell_id = 'none'
        
        return cell_label, cell_id
       
    

    def get_oneD_projection(self, x_crop, y_crop, medial_axis_df, half_window=5):
        """
        This function can be used to get the projection of the particle on the medial axis,
        as well as its euclidean distance from the centroid.
        
        Parameters
        ----------
        x_crop: float - the cropped particle coordinate along the x axis
        y_crop: float - the cropped particle coordinate along the x axis
        medial_axis_df: pandas DataFrame - the medial axis dataframe returned by the self.get_medial_axis() function (index [0])
        half_window: positive integer: half the number of knots for the window used to estimate the instantaneous medial axis slope (with a 0.01 px per knot resolution)
        
        Returns
        -------
        min_index: the index of the medial axis knot closest to the particle position
        min_arch_length: the arch length of the medial axis closest to the particle position
        min_arch_centered_length: the arch length of the medial axis closest to the particle position around the centroid
        min_arch_scaled_length: the relative (from -1 to 1) arch length of the medial axis closest to the particle position
        min_distance: the minimum distance of the particle position from the medial axis (the cross product determines the sign)
        min_distance_abs: the absolute minimum distance of the particle position from the medial axis
        centroid_euc_distance: the euclidean distance of the particle from the central line
        medial_axis_coords: the coordinates of the medial axis knot which is closest to the particle position
        """
        medial_axis_df['particle_distance'] = np.sqrt((medial_axis_df.cropped_x - x_crop)**2 + (medial_axis_df.cropped_y - y_crop)**2)
        min_df = medial_axis_df[medial_axis_df.particle_distance == medial_axis_df.particle_distance.min()]
        min_index = min_df.index.values[0]
        min_arch_length = min_df.arch_length.values[0]
        min_arch_centered_length = min_df.arch_length_centered.values[0]
        min_arch_scaled_length =  min_df.arch_length_scaled.values[0]
        min_distance_abs = min_df.particle_distance.values[0]
        medial_axis_coords = (min_df.cropped_x.values[0], min_df.cropped_y.values[0])
        
        def get_relative_distance(min_distance_abs, medial_axis_df, min_index, medial_axis_coords, x_crop, y_crop, half_window):
            """
            This function is used to estimate the direactional distance from the medial axis.
            This is a recursive algorithm.
            
            Parameters
            ----------
            medial_axis_df: pandas DataFrame - zeroth index product of the self.get_medial_axis function
            min_index: integer - the index of the medial axis coordiantes (in the medial_axis_df) with the minimum distance from the particle position
            x_crop: float - the cropped x coordinates of the particle
            y_crop: float - the cropped y coordiantes of the particle
            half_window: integer - the number of points upstream and downstream the medial axis position used to get the medial axis vecotr.
            medial_distance_abs: positive float - the absolute minimum distance of the particle position from the medial axis.
            medial_axis_coords: tuple of floats - (x,y) cropped coordinates of the medial axis position that is closest to the particle position (x_crop, y_crop)
            Returns
            -------
            min_distance: float - the directional minimum distance of the particle from the medial axis
                It is positive out and around the medial axis and negative within the medial axis octagon in a hypothetical donut shaped cell
            """
            #            half_window = 2
            if min_index>=half_window and min_index<medial_axis_df.index.max()-half_window:
                index_range = (min_index-half_window, min_index+half_window)
            elif min_index<half_window and min_index<medial_axis_df.index.max()-half_window:
                index_range = (0, min_index+half_window)
            elif min_index>=half_window and min_index>=medial_axis_df.index.max()-half_window:
                index_range = (min_index-half_window, medial_axis_df.index.max())
            
            delta_x = (medial_axis_df.iloc[index_range[1]].cropped_x -  medial_axis_df.iloc[index_range[0]].cropped_x)
            delta_y = (medial_axis_df.iloc[index_range[1]].cropped_y -  medial_axis_df.iloc[index_range[0]].cropped_y)
            medial_axis_vector = [delta_x, delta_y]
            
            particle_delta_x = x_crop - medial_axis_coords[0]
            particle_delta_y = y_crop - medial_axis_coords[1]
            particle_vector = [particle_delta_x, particle_delta_y]
            
            cross_product = np.cross(medial_axis_vector, particle_vector)
            if cross_product != 0:
                min_distance = np.sign(cross_product)*min_distance_abs
                return min_distance
            elif cross_product == 0:
                half_window+=1
                get_relative_distance(min_distance_abs, medial_axis_df, medial_axis_coords, x_crop, y_crop, half_window)
            
        min_distance = get_relative_distance(min_distance_abs, medial_axis_df, min_index, medial_axis_coords, x_crop, y_crop, half_window)

        return min_index, min_arch_length, min_arch_centered_length, min_arch_scaled_length, min_distance, min_distance_abs, medial_axis_coords[0], medial_axis_coords[1]

    

    def apply_particle_projection(self, gaussian_center, cell_id, position, medial_axis_dict):
        
        if cell_id in medial_axis_dict[position] and cell_id in self.mask_arrays[position][1]:
            
            crop_pad = self.mask_arrays[position][1][cell_id]
            x_crop = gaussian_center[0]-crop_pad[0]
            y_crop = gaussian_center[1]-crop_pad[1]
            medial_axis_df = medial_axis_dict[position][cell_id][0]
            
            return self.get_oneD_projection(x_crop, y_crop, medial_axis_df, half_window=5)
        
        else:
            return 'none','none','none','none','none','none','none','none'


        
    def getting_the_particles_in_snapshots(self, log_adaptive_parameters, min_particle_size, max_particle_size, min_particle_aspect_ratio, 
                              post_processing_threshold, box_size, 
                              metric, operation, channel, 
                              gaussian_fit_show = False):
        """
        This function is used to run the segmentation of the particles and track them.
        Gaussian distributions are fitted to the difraction limited particles to estimate the volume and the center of each particle. 
        
        Parameters
        ----------
        analysis_range: tuple of non-negaive integers: the first and the last frame to be analyzed
        log_adaptive_parameters: list of the particle segmentation parameters. See particle_segmentation function.
        min_particle_size: Non-negative integer: the minimum expected particle size in pixels. 
        max_particle_size: Positive integer: the maximum expected particle size in pixels.
        min_particle_aspect_ratio: float: the minimum expected particle aspect ration (minor/major axis length)
        post_processing_threshold: float or integer: the % threshold of brightest pixels to separate clustered spots.
        box_size: odd integer - the size of the box that is used to fit the 2D gaussian for the particle center estimation (5 or 11 suggested)
        metric: string, the column of the pandas dataframe which is used as a proxy for particle size. Choose 'gaussian volume', 'raw pixels' or 'smoothed pixels'
        operation: function, the mathematical operation applied to the bin_column as a particle size proxy. Choose 'mean', 'median', 'sum' or 'max'
    
        Returns
        -------
        A dataframe with the following fields:
            'experiment' - A string with the experiment ID
            'position_string' - string xy position of the experiment
            'particle_center' - the particle center approximated during particle segmentation (particle mask centroid)
            'max_fluorescence' - the maxmimum fluorescence of the particle estimated from the particle mask
            'particle_brightest_pixels' - the 60% of the particle pixels around the particle centroid
            'smoothed_brightest_pixels' - the 60% brightest smoothed particle pixels (after fitting a 2D Gaussian) around the particle centroid
            'gaussian_center' - the center of the particle estimated from the peak of the 2D gaussian fitted
            'gaussian_amplitude' - the amplitude of the fitted 2D gaussian
            'gaussian_std' - the std of the fitted 2D gaussian in the x and y dimension (tuple)
            'gaussian_volume' - the volume of the fitted 2D gaussian given by this function (2 Ï A Ïx Ïy) - https://en.wikipedia.org/wiki/Gaussian_function#Two-dimensional_Gaussian_function
            'gaussian_rotation' - the rotation of the fitted 2D gaussian
            'particle_fluorescence' - the proxy of particle size estimated using a mathematical operation (parameter: operation) on a given particle metric (parameter: metric)
            'average_background_fluorescence' - the average estimated background fluorescence
            'cell_id' - string
            'cell_label' - integer
            'projection_indec' - integer indicating the index of the particle projection on the medial axis
            'arch_length' - float indicating the arch length of the particle projection on the medial axis
            'arch_centered_length' - float indicating the arch length (distance from the cell center) of the particle projection on the medial axis
            'scaled_length' - float indicating the relative arch length (distance from the cell center) of the particle projection on the medial axis
            'distance' - float indicating the distance of a particle position from the medial axis
            'abs_distance' - float indicating theabsolute distance of a particle position from the medial axis
            'medial_axis_x' - the x coordinates of the particle positio projection on the medial axis
            'medial_axis_y' - the y coordinates of the particle positio projection on the medial axis
        
        This dataframe is also saved in the designated folder:
            self.save_path+'/'+self.experiment+'_'+self.position_string+'_particle_df'
        
        Raises
        -----
        ValueError if the box_size is not an odd integer (for fitting the 2D Gaussian)
                   if the metric input is not valid (for the estimation of the particle fluorescence)
                   if the operation input is not valid (for the estimation of the particle fluorescence)
        """
        if box_size%2 == 0:
            raise ValueError('The box_size parameter is even. Choose an odd integer.')
        if metric not in ['gaussian volume', 'raw pixels', 'smoothed pixels']:
            raise ValueError("Choose 'gaussian volume', 'raw pixels', or 'smoothed pixels' as a metric")
        if operation not in ['mean', 'median', 'sum', 'max']:
             raise ValueError("Choose 'mean', 'median', 'sum' or 'max' as the operation.")
        
        particle_df = pd.DataFrame(columns=['experiment', 'xy_position', 'position_string', 'particle_center', 'max_fluorescence', 'particle_brightest_pixels', 'smoothed_brightest_pixels', 'gaussian_center', 'gaussian_amplitude', 'gaussian_std', 'gaussian_volume', 'gaussian_rotation', 'particle_fluorescence', 'average_background_fluorescence'])
        
        particle_images = self.image_arrays[channel]
        phase_images = self.image_arrays['Trans']
        
        # number_of_positions = len(list(particle_images.keys()))
        
        for pos in particle_images:
            if pos < 9:
                position_string = 'xy0'+str(pos+1)
            elif pos >= 9:
                position_string = 'xy'+str(pos+1)
            
            print('getting particles for experiment',self.experiment,'and position',position_string)
            # get the signal image
            background_subtraction = self.back_sub(phase_images[pos][2], particle_images[pos][2], 25, 128, 60, False)
            bkg_cor_image = background_subtraction[0]
            mean_bkg = background_subtraction[1]
            #--------- PARTICLE SEGMENTATION ----------#
            particle_segmentation_df = self.particle_segmentation(bkg_cor_image, log_adaptive_parameters, min_particle_size, max_particle_size, min_particle_aspect_ratio, post_processing_threshold)
            for index, row in particle_segmentation_df.iterrows():
                particle_center = row['centroid']
                # get the particle center and the 2D Gaussian parameters
                particle_center_stats = self.estimate_particle_center(bkg_cor_image, particle_center, box_size, gaussian_fit_show)
                if particle_center_stats != 'none':
                    gaussian_particle_center, brightest_raw_pixels, brightest_fitted_pixels, param = particle_center_stats
                    (height, y, x, width_y, width_x, rotation) = param
                    gaussian_vol = 2*np.pi*height*width_x*width_y
                    # get the particle fluorescence metric
                    particle_vol = self.get_particle_fluorescence(metric, operation, brightest_raw_pixels, brightest_fitted_pixels, gaussian_vol)
                    # get the cell ID of the associated particle
            
                    # organize the data into a new pandas row
                    pre_df = [self.experiment, pos, position_string, particle_center, brightest_raw_pixels.max(), brightest_raw_pixels, brightest_fitted_pixels, gaussian_particle_center, height, (width_x, width_y), gaussian_vol, rotation, particle_vol, mean_bkg]
                    # append the row to the pandas dataframe
                    particle_df = particle_df.append(pd.DataFrame([pre_df], columns=list(particle_df.columns)),ignore_index=True)
        
        print('appying the cell IDs to the particle positions...')
        particle_df['cell_ids'] = particle_df.apply(lambda x: self.get_cell_with_particle(x.gaussian_center, x.xy_position, x.position_string), axis=1)
        particle_df[['cell_label','cell_id']]=pd.DataFrame(particle_df.cell_ids.tolist(), index=particle_df.index)
        particle_df = particle_df.drop('cell_ids', axis=1)
        
        
        print('getting the relative cell coordinates of the particle positions...')
        with open(self.save_path+'/'+self.experiment+'_medial_axis_dict', 'rb') as handle:
            medial_axis_dict = pickle.load(handle)
        particle_df['1d_stats'] = particle_df.apply(lambda x: self.apply_particle_projection(x.gaussian_center, x.cell_id, x.xy_position, medial_axis_dict), axis=1)
        oned_stats_list = ['projection_indec', 'arch_length', 'arch_centered_length', 'scaled_length',
                           'distance', 'abs_distance', 'medial_axis_x', 'medial_axis_y']
        particle_df[oned_stats_list] = pd.DataFrame(particle_df['1d_stats'].tolist(), index=particle_df.index)
        particle_df = particle_df.drop('1d_stats', axis=1)
        
        
        with open(self.save_path+'/'+self.experiment+'_particles_df', 'wb') as handle:
            particle_df.to_pickle(path = handle, compression='zip', protocol = pickle.HIGHEST_PROTOCOL)
        
        return particle_df
    

    
    
    